{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79982925-474e-4ca3-b265-2f96c58449f8",
   "metadata": {},
   "source": [
    "## Validating FAISS vector database library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7455db-a5c0-46ac-979b-a50c28a30ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eea1f8-0646-4580-86ba-9de020422110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 6\n",
    "index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18878ea9-cfca-4959-93d0-0f1b2c96bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numVectors = 5\n",
    "vectors = np.random.rand(numVectors, dimension).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145528a7-a3b3-4be5-991f-282bb0323855",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(vectors)\n",
    "print(f\"Number of vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983a9ff-1d10-4eea-a2d0-07a99ab366e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 5 # number of centroids\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "indexIVF = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "if not indexIVF.is_trained:\n",
    "    indexIVF.train(vectors)\n",
    "indexIVF.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fc510-856b-4dd2-9533-8864aefd1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVector = np.random.rand(1, dimension).astype('float')\n",
    "k = 5\n",
    "distances, indices = index.search(queryVector, k)\n",
    "print(f\"Distances: {distances}\")\n",
    "print(f\"Indices of nearest neighbors: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5eb0f-7a46-42cc-bf43-13904ed434d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {queryVector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16411b-fae3-4f0a-b3f7-fbeb3992b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reference vectors\")\n",
    "print(indices)\n",
    "for ind in indices[0]:\n",
    "    referenceVector = vectors[ind]\n",
    "    l2_distance = np.linalg.norm(referenceVector - queryVector)\n",
    "    print(f\"{referenceVector}: {l2_distance} compared to {distances[0][ind]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb5fce-f37e-46ec-a9db-5bb9ebe3827b",
   "metadata": {},
   "source": [
    "## Utilizing PyMuPDF to extract text from PDF for vector encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2572f-29c1-4bd0-a08f-591fe9ad2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1007e-e688-4393-b1b3-45a45d70c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6210f-4d5e-4402-8587-4f46b3da8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d7c58-1d13-4ea8-b53f-4cf1b073c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_array(arr, kernel_size=1):\n",
    "    n = len(arr)\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - kernel_size)\n",
    "        end = min(n, i + kernel_size + 1)\n",
    "        window = arr[start:end]\n",
    "        result[i] = 1.0 if np.any(window == 1) else 0.0\n",
    "    return result\n",
    "\n",
    "def erode_array(arr, kernel_size=1):\n",
    "    n = len(arr)\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - kernel_size)\n",
    "        end = min(n, i + kernel_size + 1)\n",
    "        window = arr[start:end]\n",
    "        result[i] = 1.0 if np.all(window == 1) else 0.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cc9bb-d8b8-444f-9cf5-494aa808c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, this seems to work across a few different types of PDF files\n",
    "# With more investigation, I would try turning this into a DP problem where up to some set percentage (maybe 15%) CAN be classified \n",
    "# as an \"Introduction\" section, thus allowing each page to bid for their slot and disincentivizing other pages.\n",
    "def GetTableOfContentsEstimator(linksPerPage, pageCount, lookback=5):\n",
    "    if pageCount <= lookback: return np.zeros(pageCount)\n",
    "    # look for local maxima in sliding window over linksPerPage to identify candidates for 'table of contents' sections\n",
    "    tableOfContentsEstimator = np.zeros(pageCount - lookback)\n",
    "    averageLinks = np.mean(linksPerPage)\n",
    "    for i in range(lookback, len(linksPerPage)):\n",
    "        pastAverage = np.mean(linksPerPage[(i - lookback):i])\n",
    "        current = linksPerPage[i]\n",
    "        if current <= pastAverage:\n",
    "            dropRatio = max((pastAverage - current), 1) / (pastAverage + 1e-6)\n",
    "            magnitude = pastAverage / (pastAverage + averageLinks + 1e-6)\n",
    "            frontBias = np.exp(-(5.0/pageCount) * (i - lookback)) # bias towards zero at the end of the array\n",
    "            tableOfContentsEstimator[i - lookback] = dropRatio * magnitude * frontBias\n",
    "        else:\n",
    "            tableOfContentsEstimator[i - lookback] = 0.0\n",
    "    binaryResult = np.where(tableOfContentsEstimator >= 0.5, 1, 0)\n",
    "    # perform morphological closing with k=1\n",
    "    binaryResult = dilate_array(erode_array(binaryResult, 1), 1)\n",
    "    # return last position where binaryResult == 1, this is likely the final page of the introduction / table of contents\n",
    "    for i in range(len(binaryResult) - 1, -1, -1):\n",
    "        if binaryResult[i] == 1:\n",
    "            return i + lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7667d9-2259-4ecd-acbf-70e000a84e4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = fitz.open(\"Observability-Engineering.pdf\")\n",
    "pageCount = doc.page_count\n",
    "\n",
    "# metadata\n",
    "linksPerPage = []\n",
    "blocksPerPage = []\n",
    "linksToPage = {}\n",
    "maxPageContentDimensions = [np.inf, np.inf, -np.inf, -np.inf]\n",
    "# once-over to compute metadata before storing any paragraph info\n",
    "for pageIndex in range(doc.page_count):\n",
    "    page = doc.load_page(pageIndex)\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    links = page.get_links()\n",
    "    linksPerPage.append(len(links))\n",
    "    blocksPerPage.append(len(blocks))\n",
    "    for block in blocks:\n",
    "        maxPageContentDimensions[:2] = np.minimum(maxPageContentDimensions[:2], block[:2])\n",
    "        maxPageContentDimensions[2:] = np.maximum(maxPageContentDimensions[2:4], block[2:4])\n",
    "    for link in links:\n",
    "        if not 'page' in link: continue\n",
    "        linkTo = link['page']\n",
    "        linksToPage[linkTo] = linksToPage.get(linkTo, 0) + 1\n",
    "\n",
    "# approximate which pages may be part of the introduction / table of contents\n",
    "lastTableOfContentsPage = GetTableOfContentsEstimator(linksPerPage, pageCount, 5) # underlying array looks something like [1,1,1,1,0,0,0,0,0,0,0,0,0,0]\n",
    "print(lastTableOfContentsPage)\n",
    "\n",
    "# estimate which are the most likely content pages\n",
    "if len(linksToPage) != 0:\n",
    "    startingContentPage = np.min(list(linksToPage.keys()))\n",
    "    endingContentPage = np.max(list(linksToPage.keys()))\n",
    "    likelyContentPages = np.zeros(pageCount, dtype=int)\n",
    "    likelyContentPages[startingContentPage:endingContentPage] = 1\n",
    "else:\n",
    "    likelyContentPages = np.ones(pageCount, dtype=int)\n",
    "print(likelyContentPages)\n",
    "\n",
    "# get metric data on blocks per page\n",
    "averageBlocksPerPage = np.mean(blocksPerPage)\n",
    "print(averageBlocksPerPage)\n",
    "\n",
    "all_sentences = []\n",
    "for pageIndex in range(doc.page_count):\n",
    "    page = doc.load_page(pageIndex)\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    if lastTableOfContentsPage and pageIndex < lastTableOfContentsPage:\n",
    "        continue\n",
    "    if not likelyContentPages[pageIndex]:\n",
    "        if len(blocks) == 0 or len(blocks) < averageBlocksPerPage / 2:\n",
    "            continue\n",
    "    for block in blocks:\n",
    "        paragraph_text = block[4].strip().replace(\"\\n\", \" \")\n",
    "        if paragraph_text.isnumeric() or len(paragraph_text) < 10: continue # hacky, but remove all very-short phrases as they're likely not substantial content\n",
    "        all_sentences.append(paragraph_text)\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb29262-053d-4c43-8e74-090809861971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a386c1-a6f3-44f6-9321-679b7d650072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group adjacent sentences depending on punctuation\n",
    "def sentence_groups(lines):\n",
    "    group = []\n",
    "    for w in lines:\n",
    "        strippedW = w.strip()\n",
    "        parts = re.split(r'([.?!])', strippedW)\n",
    "        for part in parts:\n",
    "            if part == '.' or part == '?' or part == '!' and group:\n",
    "                yield group\n",
    "                group = []\n",
    "            if not part.isnumeric() and len(part) >= 10:\n",
    "                group.append(part)\n",
    "    if group:\n",
    "        yield group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4479b13-d8dc-4bcb-9988-6b79e5cf1271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_sentences(text_list):\n",
    "    full_text = ' '.join(text_list)\n",
    "    sentences = nltk.tokenize.sent_tokenize(full_text)\n",
    "    return [sent.strip() for sent in sentences if len(sent.strip()) > 10]\n",
    "\n",
    "sentencesProcessed = parse_sentences(all_sentences)\n",
    "for sentence in sentencesProcessed:\n",
    "    print(sentence)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f29362-0811-43a7-bec5-ff831a1eb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplifiedSentences = [' '.join(group) for group in sentence_groups(all_sentences)]\n",
    "#for sentence in simplifiedSentences:\n",
    "#    print(sentence)\n",
    "#    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf0a35-12de-4dba-a749-625d4dff4875",
   "metadata": {},
   "source": [
    "## Utilizing SBERT library to perform semantic encoding of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e27024-df8b-4197-8926-4149b7b69abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc653ec7-715d-4595-aa93-3ebd93635e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef9b1d-cf01-4c39-aab5-e72767cabc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea08a0c-cf7d-4107-bb1a-fb6257271eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode_document(sentencesProcessed, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb6ea8-1762-4346-8f1c-5bbca3acbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are the eligibility requirements for Medicare home health services?\",\n",
    "    \"What qualifies a patient as homebound under CMS guidelines?\",\n",
    "    \"What skilled nursing services are considered reasonable and necessary?\",\n",
    "    \"What conditions must be met for physical therapy to be covered?\",\n",
    "    \"What must be included in the physicianâ€™s plan of care?\",\n",
    "    \"What are the requirements for the physician face-to-face encounter?\",\n",
    "    \"What documentation is needed to prove medical necessity for skilled services?\",\n",
    "    \"How should changes to the plan of care be documented during the episode?\",\n",
    "    \"Under what conditions are home health aide services covered?\",\n",
    "    \"What are the supervision requirements for home health aides?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dab85c-37f0-4bfa-a5f6-9b3587180985",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What types of monitoring are needed for companies that run a large portion of their own systems on low-level hardware?\",\n",
    "    \"Describe some advantages of test-driven development regarding upkeep of a software product.\",\n",
    "    \"How can observability be coupled with a development effort to prevent rolling back deployments?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef743391-1e4c-4e5f-9b53-5180facf70be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "for query in queries:\n",
    "    query_embedding = model.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    similarity_scores = model.similarity(query_embedding, corpus_embeddings)[0]\n",
    "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(scores, indices):\n",
    "        context = sentencesProcessed[idx-2:idx+2]\n",
    "        print(f\"(Score: {score:.4f})\", context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a011ec-dd77-49e6-940b-02820f430ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
