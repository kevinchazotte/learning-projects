{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79982925-474e-4ca3-b265-2f96c58449f8",
   "metadata": {},
   "source": [
    "## Validating FAISS vector database library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7455db-a5c0-46ac-979b-a50c28a30ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eea1f8-0646-4580-86ba-9de020422110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 6\n",
    "index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18878ea9-cfca-4959-93d0-0f1b2c96bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numVectors = 5\n",
    "vectors = np.random.rand(numVectors, dimension).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145528a7-a3b3-4be5-991f-282bb0323855",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(vectors)\n",
    "print(f\"Number of vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983a9ff-1d10-4eea-a2d0-07a99ab366e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 5 # number of centroids\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "indexIVF = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "if not indexIVF.is_trained:\n",
    "    indexIVF.train(vectors)\n",
    "indexIVF.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fc510-856b-4dd2-9533-8864aefd1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVector = np.random.rand(1, dimension).astype('float')\n",
    "k = 5\n",
    "distances, indices = index.search(queryVector, k)\n",
    "print(f\"Distances: {distances}\")\n",
    "print(f\"Indices of nearest neighbors: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5eb0f-7a46-42cc-bf43-13904ed434d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {queryVector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16411b-fae3-4f0a-b3f7-fbeb3992b06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reference vectors\")\n",
    "print(indices)\n",
    "for ind in indices[0]:\n",
    "    referenceVector = vectors[ind]\n",
    "    l2_distance = np.linalg.norm(referenceVector - queryVector)\n",
    "    print(f\"{referenceVector}: {l2_distance} compared to {distances[0][ind]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb5fce-f37e-46ec-a9db-5bb9ebe3827b",
   "metadata": {},
   "source": [
    "## Utilizing PyMuPDF to extract text from PDF for vector encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2572f-29c1-4bd0-a08f-591fe9ad2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1007e-e688-4393-b1b3-45a45d70c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6210f-4d5e-4402-8587-4f46b3da8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d7c58-1d13-4ea8-b53f-4cf1b073c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_array(arr, kernel_size=1):\n",
    "    n = len(arr)\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - kernel_size)\n",
    "        end = min(n, i + kernel_size + 1)\n",
    "        window = arr[start:end]\n",
    "        result[i] = 1.0 if np.any(window == 1) else 0.0\n",
    "    return result\n",
    "\n",
    "def erode_array(arr, kernel_size=1):\n",
    "    n = len(arr)\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - kernel_size)\n",
    "        end = min(n, i + kernel_size + 1)\n",
    "        window = arr[start:end]\n",
    "        result[i] = 1.0 if np.all(window == 1) else 0.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cc9bb-d8b8-444f-9cf5-494aa808c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, this seems to work across a few different types of PDF files\n",
    "# With more investigation, I would try turning this into a DP problem where up to some set percentage (maybe 15%) CAN be classified \n",
    "# as an \"Introduction\" section, thus allowing each page to bid for their slot and disincentivizing other pages.\n",
    "def GetTableOfContentsEstimator(linksPerPage, pageCount, lookback=5):\n",
    "    if pageCount <= lookback: return np.zeros(pageCount)\n",
    "    # look for local maxima in sliding window over linksPerPage to identify candidates for 'table of contents' sections\n",
    "    tableOfContentsEstimator = np.zeros(pageCount - lookback)\n",
    "    averageLinks = np.mean(linksPerPage)\n",
    "    for i in range(lookback, len(linksPerPage)):\n",
    "        pastAverage = np.mean(linksPerPage[(i - lookback):i])\n",
    "        current = linksPerPage[i]\n",
    "        if current <= pastAverage:\n",
    "            dropRatio = max((pastAverage - current), 1) / (pastAverage + 1e-6)\n",
    "            magnitude = pastAverage / (pastAverage + averageLinks + 1e-6)\n",
    "            frontBias = np.exp(-(5.0/pageCount) * (i - lookback)) # bias towards zero at the end of the array\n",
    "            tableOfContentsEstimator[i - lookback] = dropRatio * magnitude * frontBias\n",
    "        else:\n",
    "            tableOfContentsEstimator[i - lookback] = 0.0\n",
    "    binaryResult = np.where(tableOfContentsEstimator >= 0.5, 1, 0)\n",
    "    # perform morphological closing with k=1\n",
    "    binaryResult = dilate_array(erode_array(binaryResult, 1), 1)\n",
    "    # return last position where binaryResult == 1, this is likely the final page of the introduction / table of contents\n",
    "    for i in range(len(binaryResult) - 1, -1, -1):\n",
    "        if binaryResult[i] == 1:\n",
    "            return i + lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b260aa9-0461-4905-88bf-50e51b27598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDocumentMetadata(document):\n",
    "    pageCount = document.page_count\n",
    "    linksPerPage = []\n",
    "    blocksPerPage = []\n",
    "    linksToPage = {}\n",
    "    # once-over to compute metadata before storing any paragraph info\n",
    "    for pageIndex in range(pageCount):\n",
    "        page = document.load_page(pageIndex)\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        links = page.get_links()\n",
    "        linksPerPage.append(len(links))\n",
    "        blocksPerPage.append(len(blocks))\n",
    "        for link in links:\n",
    "            if not 'page' in link: continue\n",
    "            linkTo = link['page']\n",
    "            linksToPage[linkTo] = linksToPage.get(linkTo, 0) + 1\n",
    "    \n",
    "    # approximate which pages may be part of the introduction / table of contents\n",
    "    lastTableOfContentsPage = GetTableOfContentsEstimator(linksPerPage, pageCount, 5) # underlying array looks something like [1,1,1,1,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    # estimate which pages are the most likely content pages based on internal links\n",
    "    if len(linksToPage) != 0:\n",
    "        startingContentPage = np.min(list(linksToPage.keys()))\n",
    "        endingContentPage = np.max(list(linksToPage.keys()))\n",
    "        likelyContentPages = np.zeros(pageCount, dtype=int)\n",
    "        likelyContentPages[startingContentPage:endingContentPage] = 1\n",
    "    else:\n",
    "        likelyContentPages = np.ones(pageCount, dtype=int)\n",
    "\n",
    "    return (lastTableOfContentsPage, likelyContentPages, np.mean(blocksPerPage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c930787-ff96-424a-abba-e5bc92426b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractSentencesFromDocument(document, metadata):\n",
    "    all_sentences = []\n",
    "    lastTableOfContentsPage = metadata[0]\n",
    "    likelyContentPages = metadata[1]\n",
    "    averageBlocksPerPage = metadata[2]\n",
    "    for pageIndex in range(document.page_count):\n",
    "        page = document.load_page(pageIndex)\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        if lastTableOfContentsPage and pageIndex < lastTableOfContentsPage:\n",
    "            continue\n",
    "        if not likelyContentPages[pageIndex]:\n",
    "            if len(blocks) == 0 or len(blocks) < averageBlocksPerPage / 2:\n",
    "                continue\n",
    "        for block in blocks:\n",
    "            paragraph_text = block[4].strip().replace(\"\\n\", \" \")\n",
    "            if paragraph_text.isnumeric() or len(paragraph_text) < 10: continue # hacky, but remove all very-short phrases as they're likely not substantial content\n",
    "            all_sentences.append(paragraph_text)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7667d9-2259-4ecd-acbf-70e000a84e4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = fitz.open(\"./../data/Observability-Engineering.pdf\")\n",
    "\n",
    "\n",
    "metadata = GetDocumentMetadata(doc)\n",
    "print(metadata)\n",
    "all_sentences = ExtractSentencesFromDocument(doc, metadata)\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb29262-053d-4c43-8e74-090809861971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a386c1-a6f3-44f6-9321-679b7d650072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group adjacent sentences depending on punctuation\n",
    "def sentence_groups(lines):\n",
    "    group = []\n",
    "    for w in lines:\n",
    "        strippedW = w.strip()\n",
    "        parts = re.split(r'([.?!])', strippedW)\n",
    "        for part in parts:\n",
    "            if part == '.' or part == '?' or part == '!' and group:\n",
    "                yield group\n",
    "                group = []\n",
    "            if not part.isnumeric() and len(part) >= 10:\n",
    "                group.append(part)\n",
    "    if group:\n",
    "        yield group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4479b13-d8dc-4bcb-9988-6b79e5cf1271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_sentences(text_list):\n",
    "    full_text = ' '.join(text_list)\n",
    "    sentences = nltk.tokenize.sent_tokenize(full_text)\n",
    "    return [sent.strip() for sent in sentences if len(sent.strip()) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6fef9-3e50-4d26-a657-807f86aac56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentencesProcessed = parse_sentences(all_sentences)\n",
    "for sentence in sentencesProcessed:\n",
    "    print(sentence)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f29362-0811-43a7-bec5-ff831a1eb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplifiedSentences = [' '.join(group) for group in sentence_groups(all_sentences)]\n",
    "#for sentence in simplifiedSentences:\n",
    "#    print(sentence)\n",
    "#    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf0a35-12de-4dba-a749-625d4dff4875",
   "metadata": {},
   "source": [
    "## Utilizing SBERT library to perform semantic encoding of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e27024-df8b-4197-8926-4149b7b69abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc653ec7-715d-4595-aa93-3ebd93635e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef9b1d-cf01-4c39-aab5-e72767cabc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea08a0c-cf7d-4107-bb1a-fb6257271eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode_document(sentencesProcessed, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb6ea8-1762-4346-8f1c-5bbca3acbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are the eligibility requirements for Medicare home health services?\",\n",
    "    \"What qualifies a patient as homebound under CMS guidelines?\",\n",
    "    \"What skilled nursing services are considered reasonable and necessary?\",\n",
    "    \"What conditions must be met for physical therapy to be covered?\",\n",
    "    \"What must be included in the physician’s plan of care?\",\n",
    "    \"What are the requirements for the physician face-to-face encounter?\",\n",
    "    \"What documentation is needed to prove medical necessity for skilled services?\",\n",
    "    \"How should changes to the plan of care be documented during the episode?\",\n",
    "    \"Under what conditions are home health aide services covered?\",\n",
    "    \"What are the supervision requirements for home health aides?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dab85c-37f0-4bfa-a5f6-9b3587180985",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What types of monitoring are needed for companies that run a large portion of their own systems on low-level hardware?\",\n",
    "    \"Describe some advantages of test-driven development regarding upkeep of a software product.\",\n",
    "    \"How can observability be coupled with a development effort to prevent rolling back deployments?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef743391-1e4c-4e5f-9b53-5180facf70be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "for query in queries:\n",
    "    query_embedding = model.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    similarity_scores = model.similarity(query_embedding, corpus_embeddings)[0]\n",
    "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(scores, indices):\n",
    "        context = sentencesProcessed[idx-2:idx+2]\n",
    "        print(f\"(Score: {score:.4f})\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c5830-bbbe-4768-842e-22c3821468ba",
   "metadata": {},
   "source": [
    "## Set up in-memory file storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22646c7-5753-4cf1-908f-94ce66033901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395a7a3-618a-4cc8-8266-7765b16305d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17364ff2-9443-4adf-b4e1-86832ce9761e",
   "metadata": {},
   "source": [
    "## Setting up API endpoints for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f2257-47c0-4c84-bccf-651427a9370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request, abort\n",
    "import uuid\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2e576-0aaa-4f25-aba6-c4925edfc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiModel = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f4ec4-f2c0-4d76-ace0-6dd3bf00a2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# POST endpoint to upload pdf\n",
    "@app.route('/upload-pdf', methods=['POST'])\n",
    "def upload_pdf():\n",
    "    if 'pdf' not in request.files:\n",
    "        return jsonify({'error': 'Bad Request', 'message': 'pdf key not found in request.files'}), 400\n",
    "    file = request.files['pdf']\n",
    "    if not file:\n",
    "        response = jsonify({'error': 'Bad Request', 'message': 'file not found in request'})\n",
    "        response.status_code = 400\n",
    "        return response\n",
    "    # Generate unique key\n",
    "    document_key = str(uuid.uuid4())\n",
    "    # Store in Redis (base64 encoded)\n",
    "    pdf_data = base64.b64encode(file.read()).decode('utf-8')\n",
    "    redis_client.setex(document_key, 3600, pdf_data)  # 1 hour expiry\n",
    "    return jsonify({\"document_key\": document_key})\n",
    "\n",
    "# POST endpoint to build document model\n",
    "@app.route('/encode-document', methods=['POST'])\n",
    "def receive_document():\n",
    "    data = request.json # Access JSON data from the request body\n",
    "    if 'document_key' not in data:\n",
    "        return jsonify({'error': 'Bad Request', 'message': 'document_key not found in request json'}), 400\n",
    "    document_key = data.get('document_key')\n",
    "    document = redis_client.get(document_key)\n",
    "    if not document:\n",
    "        return jsonify({'error': 'Not Found', 'message': 'document not found in memory storage'}), 404\n",
    "    try:\n",
    "        pdf_data = base64.b64decode(document)\n",
    "        pdf_stream = io.BytesIO(pdf_data)\n",
    "        doc = fitz.open(stream=pdf_stream, filetype=\"pdf\")\n",
    "        metadata = GetDocumentMetadata(doc)\n",
    "        apiSentences = ExtractSentencesFromDocument(doc, metadata)\n",
    "        apiSentencesProcessed = parse_sentences(apiSentences)\n",
    "        sentences_serialized = pickle.dumps(apiSentencesProcessed)\n",
    "        redis_client.setex(f\"{document_key}_sentences\", 3600, sentences_serialized)\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': f'PDF processing error: {str(e)}'}), 500\n",
    "    apiCorpusEmbeddings = apiModel.encode_document(apiSentencesProcessed)\n",
    "    try:\n",
    "        embeddings_bytes = pickle.dumps(apiCorpusEmbeddings)\n",
    "        # Store in Redis with the same document_key\n",
    "        redis_client.setex(f\"{document_key}_embeddings\", 3600, embeddings_bytes)\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': f'Storage error: {str(e)}'}), 500\n",
    "    return jsonify({\"status\": \"success\"})\n",
    "\n",
    "# POST endpoint to query model\n",
    "@app.route('/query', methods=['POST'])\n",
    "def hello_world():\n",
    "    data = request.json\n",
    "    if 'query' not in data or 'document_key' not in data:\n",
    "        return jsonify({'error': 'Bad Request', 'message': 'query not found in request json'}), 400\n",
    "    query = data.get('query')\n",
    "    document_key = data.get('document_key')\n",
    "    document_embeddings = redis_client.get(f\"{document_key}_embeddings\")\n",
    "    document_sentences = redis_client.get(f\"{document_key}_sentences\")\n",
    "    if not document_embeddings or not document_sentences:\n",
    "        return jsonify({'error': 'Not Found', 'message': 'document embeddings not found in memory storage'}), 404\n",
    "    apiCorpusEmbeddings = pickle.loads(document_embeddings)\n",
    "    apiSentencesProcessed = pickle.loads(document_sentences)\n",
    "    top_k = 5\n",
    "    apiQueryEmbedding = apiModel.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "    # use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cosineSimilarities = apiModel.similarity(apiQueryEmbedding, apiCorpusEmbeddings)[0]\n",
    "    topScores, topIndices = torch.topk(cosineSimilarities, k=top_k)\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "    outputString = \"\"\n",
    "    for score, idx in zip(topScores, topIndices):\n",
    "        if idx:\n",
    "            context = apiSentencesProcessed[max(0, idx-2):min(len(apiSentencesProcessed), idx+2)]\n",
    "            outputString = outputString + f\"(Score: {score:.4f})\" + ' '.join(context)\n",
    "            print(f\"(Score: {score:.4f})\", context)\n",
    "    return jsonify({\"status\": \"success\", \"output\": outputString})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3826112-c28a-4ba8-871f-b7ca5dcba967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
